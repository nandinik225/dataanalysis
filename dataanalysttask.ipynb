{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataanalysttask.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB-1teB5Zv-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd0768d2-4065-4c54-e6ce-d7d6f1714b8f"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"nps_chat\")\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.corpus import nps_chat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqq8vGlKRbGJ"
      },
      "source": [
        "\n",
        "def dialogue_act_features(post):\n",
        "    features = {}\n",
        "    for word in nltk.word_tokenize(post):\n",
        "        features['contains({})'.format(word.lower())] = True\n",
        "    return features\n",
        "featuresets = [(dialogue_act_features(post.text), post.get('class')) for post in posts]\n",
        "size = int(len(featuresets) * 0.1)\n",
        "train_set, test_set = featuresets[size:], featuresets[:size]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "question_types = [\"whQuestion\",\"ynQuestion\"]\n",
        "def is_ques_using_nltk(ques):\n",
        "    question_type = classifier.classify(dialogue_act_features(ques)) \n",
        "    return question_type in question_types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWB47UaURcHV"
      },
      "source": [
        "question_pattern = [ \"what\", \"who\", \"is it\", \"why\",\"would you\", \"how\",\"is there\",\"did\",\n",
        "                    \"are there\", \"is it so\", \"is this true\" ,\"to know\", \"is that true\", \"are we\", \"am i\",\"do i\", \"do you\", \n",
        "                   \"question is\", \"tell me more\", \"can i\", \"can we\", \"tell me\", \"can you explain\",\n",
        "                   \"question\",\"answer\", \"questions\", \"answers\", \"ask\",\"quien\", \"que\",\"donde\",\"cuando\",\"cual\",\"por que\",\"como\",\"o que\",\"quem\",\"onde\",\"\tquanto\",\"\tonde\"]\n",
        "\n",
        "helping_verbs = [\"is\",\"am\",\"can\", \"are\", \"do\", \"does\"]\n",
        "def is_question(question):\n",
        "    question = question.lower().strip()\n",
        "    if not is_ques_using_nltk(question):\n",
        "        is_ques = False\n",
        "        for pattern in question_pattern:\n",
        "            is_ques  = pattern in question\n",
        "            if is_ques:\n",
        "                break\n",
        "        sentence_arr = question.split(\".\")\n",
        "        for sentence in sentence_arr:\n",
        "            if len(sentence.strip()):\n",
        "                first_word = nltk.word_tokenize(sentence)[0]\n",
        "                if sentence.endswith(\"?\") or first_word in helping_verbs:\n",
        "                    is_ques = True\n",
        "                    break\n",
        "        return is_ques    \n",
        "    else:\n",
        "        return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_yQ9NEJiU9V"
      },
      "source": [
        "with open('data.txt', mode='r') as in_file, \\\n",
        "     open('dataanalysis.txt', mode='w') as out_file:\n",
        "\n",
        "     for line in in_file:\n",
        "       if is_question(line):\n",
        "         out_file.write(line+\"yes\"+\"\\n\")\n",
        "       else:\n",
        "         out_file.write(line+\"no\"+\"\\n\")\n",
        "         \n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}